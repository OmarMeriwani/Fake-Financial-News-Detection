{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dataset Preparation - Scrapper.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmarMeriwani/Fake-Financial-News-Detection/blob/master/Dataset_Preparation_Scrapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEmDfBWiP5rn",
        "colab_type": "text"
      },
      "source": [
        "# Dataset Preparation - Scrapper\n",
        "This document contains the code for snopes.com scrapper to get business news archived articles.\n",
        "The work is mainly done using BeautifulSoup python library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u2jhKzpQjBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup, NavigableString, Tag\n",
        "from urllib.request import urlopen"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWVeFU4DQjyz",
        "colab_type": "text"
      },
      "source": [
        "The link below shows the path for the business news archive in snopes.com website, it contains 38 pages each of which has links to 10 articles that contain claims, status of the claim and date. These details has been declared in a pandas dataframe to be stored."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWgxwG2DQj6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://www.snopes.com/fact-check/category/business/page/NUMBER/\n",
        "#From 1-38\n",
        "linkss = []\n",
        "df = pd.DataFrame(columns=['Link','Claim','Status','Date'])\n",
        "seq = 0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oz0jyZS6Qj__",
        "colab_type": "text"
      },
      "source": [
        "We used two versions of scrapper code (2 and 3 sections in the code), as the way of writing articles have been changed during the time and the HTML code looks different. Each version gets specific HTML tags and relies on classes, IDs and HTML tag types to get the required information. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1DGbxyPQkFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range (10,38):\n",
        "    Page1 = urlopen(url='https://www.snopes.com/fact-check/category/business/page/' + str(i), data=None)\n",
        "    Page1 = BeautifulSoup(Page1, features=\"html5lib\")\n",
        "    articles = Page1.find_all(lambda tag: tag and tag.name.startswith(\"article\"))\n",
        "    #print(articles)\n",
        "    for a in articles:\n",
        "        if a == None:\n",
        "            continue\n",
        "        link = a.find_all(lambda tag: tag and tag.name == \"a\" ,href=True)\n",
        "        try:\n",
        "\n",
        "            link = link[0]['href']\n",
        "            print('LINK:',link)\n",
        "            # Enter the link\n",
        "            PageInside = urlopen(url=link, data=None)\n",
        "            PageInside = BeautifulSoup(PageInside, features=\"html5lib\")\n",
        "            # Find p with \"Claim:\"\n",
        "            claim = PageInside.find('div',{'class': 'content'}).find_all('p')[0]\n",
        "            print('Claim:', claim)\n",
        "\n",
        "\n",
        "            #=================================================2==============================\n",
        "\n",
        "            claim = ''\n",
        "            claim = PageInside.find_all(lambda tag: tag and tag.name == \"span\" and tag.text == \"Claim\")[0].nextSibling\n",
        "            print('Claim:',claim)\n",
        "            if claim.strip() == '':\n",
        "                continue\n",
        "            statusTag = PageInside.find('noindex').find(\"span\").find(\"span\").next\n",
        "            print(statusTag)\n",
        "            publishDate = PageInside.find_all(lambda tag: tag and tag.name == \"span\" and tag.text == \"Originally published:\")[0].nextSibling\n",
        "            print('DATE: ',publishDate)\n",
        "\n",
        "            '''\n",
        "            #=================================================3==============================\n",
        "            claimTag = PageInside.find(\"div\", {\"class\": \"claim\"}).find('p')\n",
        "            claim = claimTag.next\n",
        "            print('Claim:',claim)\n",
        "            statusTag = PageInside.find(\"div\", {\"class\": \"rating-wrapper card\"}).find(\"div\", {\"class\": \"media-list\"}).find(\"div\", {\"class\":\"media rating\"}).find(\"div\",{\"class\":\"media-body\"}).find(\"h5\").next\n",
        "            # Find p with \"Status:\"\n",
        "            # Find p with \"Last updated:\"\n",
        "            print('Status:', statusTag)\n",
        "            publishDate = PageInside.find(\"div\",{\"class\":\"footer-wrapper\"}).find(\"footer\").find(\"ul\").find(\"li\",{\"class\":\"date date-published\"}).find_all(\"span\")[1].next\n",
        "            print(\"Date: \", publishDate)\n",
        "            '''\n",
        "            df.loc[seq] = [link, claim, str(statusTag), str(publishDate)]\n",
        "            seq += 1\n",
        "            df.to_csv('snopes_dataset3.csv')\n",
        "        except Exception as e:\n",
        "            print(e)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}