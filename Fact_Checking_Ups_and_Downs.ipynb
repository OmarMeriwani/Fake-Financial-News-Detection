{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fact Checking - Ups and Downs.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmarMeriwani/Fake-Financial-News-Detection/blob/master/Fact_Checking_Ups_and_Downs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_vAtvgY0NVI",
        "colab_type": "text"
      },
      "source": [
        "# Fact Checking - Ups and Downs\n",
        "This document contains the source code for the ups and downs classifier, which is used to specify whether the news titles are mentioning news that lead to higher or lower stock market measures for a specific company."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tm3osgty0NcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from string import punctuation\n",
        "import pandas as pd\n",
        "from numpy import zeros\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential, Model\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import datapath\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.stem.porter import *\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding, GlobalMaxPooling1D\n",
        "from keras.layers.merge import Concatenate\n",
        "from Vocabulary import clean_doc\n",
        "from keras.utils import np_utils\n",
        "import os\n",
        "from stanfordcorenlp import StanfordCoreNLP\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjOryDPw0NiR",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing tools definitions and Stanford Core NLP tool explained [here](https://github.com/OmarMeriwani/Fake-Financial-News-Detection/blob/master/Final/Objectivity/News_Sources_Analysis_Who_Said.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkU94-hN0NoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "java_path = \"C:/Program Files/Java/jdk1.8.0_161/bin/java.exe\"\n",
        "os.environ['JAVAHOME'] = java_path\n",
        "host='http://localhost'\n",
        "port=9000\n",
        "scnlp =StanfordCoreNLP(host, port=port,lang='en', timeout=30000)\n",
        "stemmer = PorterStemmer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6mBTaTE0OD8",
        "colab_type": "text"
      },
      "source": [
        "This method finds if a specific word exists in the vocabulary, then it assigns vector weights extracted from a ready embeddings to make the semantic vector representations. The words that does not exist in the vocabulary are replaced with zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4C09eSf0OJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_weight_matrix2(embedding, vocab):\n",
        "    vocab_size2 = len(vocab) + 1\n",
        "    weight_matrix = zeros((vocab_size2, 300))\n",
        "    for word, i in vocab:\n",
        "        vector = None\n",
        "        try:\n",
        "            vector = embedding.get_vector(word)\n",
        "        except:\n",
        "            continue\n",
        "        if vector is not None:\n",
        "            weight_matrix[i] = vector\n",
        "    return weight_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3koNvX8c0OPM",
        "colab_type": "text"
      },
      "source": [
        "The method below reads a datasheet file, and performs the following tasks:\n",
        "* Get the news title and the stock market effect from each row.\n",
        "* Replace named entities.\n",
        "* Replace numbers.\n",
        "* Remove stop words.\n",
        "* Remove punctuation.\n",
        "* Get POS Tags.\n",
        "* Then create an array of sentences.\n",
        "* Convert the effect to either 1 or 0 (original values represents a range between -1 and 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIrhFFhI0OVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readfile(filename):\n",
        "    df = pd.read_csv(filename,header=0)\n",
        "    mode = 'sentence'\n",
        "    data = pd.DataFrame(columns=['title','effect'])\n",
        "    prev = ''\n",
        "    seq = 0\n",
        "    table = str.maketrans('', '', punctuation)\n",
        "\n",
        "    for i in range(0,len(df)):\n",
        "        sentence = df.loc[i][3]\n",
        "        company = str(df.loc[i][2]).lower()\n",
        "\n",
        "        tokens = scnlp.word_tokenize(sentence)\n",
        "        sentenceList = []\n",
        "        for word in tokens:\n",
        "            #print(word)\n",
        "            isAllUpperCase = True\n",
        "            for letter in word:\n",
        "                if letter.isupper() == False:\n",
        "                    isAllUpperCase = False\n",
        "                    break\n",
        "\n",
        "            if isAllUpperCase == False:\n",
        "                sentenceList.append(str(word))\n",
        "            else:\n",
        "                sentenceList.append('#ner')\n",
        "        tokens = sentenceList\n",
        "\n",
        "        tokens = [w.translate(table) for w in tokens]\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [w for w in tokens if not w in stop_words]\n",
        "        # filter out short tokens\n",
        "        tokens = [lemmatizer.lemmatize(word.lower()) for word in tokens if len(word) > 1]\n",
        "        sentence = ' '.join(tokens)\n",
        "        NER = scnlp.ner(str(sentence))\n",
        "        POS = scnlp.pos_tag(str(sentence).lower())\n",
        "\n",
        "        sentenceList = []\n",
        "        for i in range(0,len(NER)):\n",
        "            w = NER[i][0]\n",
        "            n = NER[i][1]\n",
        "            pos = NER[i][1]\n",
        "            #print(w, n)\n",
        "            if str(w).isnumeric() == True:\n",
        "                sentenceList.append('#num')\n",
        "                continue\n",
        "            if pos == 'NNP' and w != '#ner':\n",
        "                sentenceList.append('#ner')\n",
        "                continue\n",
        "            if str(n) == 'O' :\n",
        "                sentenceList.append(w)\n",
        "            else:\n",
        "                sentenceList.append('#ner')\n",
        "        sentence = ' '.join(sentenceList)\n",
        "        effect = df.loc[i][4]\n",
        "        if effect > 0:\n",
        "            effect = 1\n",
        "        else:\n",
        "            effect = 0\n",
        "        if sentence.strip() != '':\n",
        "            data.loc[seq] = [sentence,effect]\n",
        "            print(sentence, effect)\n",
        "            seq += 1\n",
        "    return data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbKTTGCT97bn",
        "colab_type": "text"
      },
      "source": [
        "Read the dataset, split training and testing samples and convert the labels into categorical output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khQhpIAc97AI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = readfile('SSIX News headlines Gold Standard EN.csv')\n",
        "headlines = data[['title']]\n",
        "effects = data[['effect']]\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(headlines,effects,test_size=0.2)\n",
        "traindata = np.array(x_train)\n",
        "testdata = np.array(x_test)\n",
        "\n",
        "y_testold = y_test\n",
        "y_test = np_utils.to_categorical(y_test,num_classes=2)\n",
        "print(y_testold, y_test)\n",
        "y_train = np_utils.to_categorical(y_train,num_classes=2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3lJGBBQ9607",
        "colab_type": "text"
      },
      "source": [
        "Prepare the data for word2vec vectors by converting the text into sequences and perform padding sequences to limt them by the minimum length of news titles. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA5PSBGd98yZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_docs = traindata[:,0]\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_docs)\n",
        "\n",
        "test_docs = testdata[:,0]\n",
        "# pad sequences\n",
        "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "print('max_length', max_length)\n",
        "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "\n",
        "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
        "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "\n",
        "# define vocabulary size (largest integer value)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvUmvm8B99EJ",
        "colab_type": "text"
      },
      "source": [
        "Get weight vectors from Google news vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj5IOe_v-Gxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wv_from_bin = KeyedVectors.load_word2vec_format(datapath('E:/Data/GN/GoogleNews-vectors-negative300.bin'), binary=True)\n",
        "embedding_vectors = get_weight_matrix2(wv_from_bin, tokenizer.word_index.items())\n",
        "\n",
        "print('embedding_vectors.shape() =============================')\n",
        "print(embedding_vectors.shape)\n",
        "\n",
        "# create the embedding layer\n",
        "embedding_layer = Embedding(vocab_size, 300, weights=[embedding_vectors], input_length=max_length, trainable=False)\n",
        "# define model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1pZh2Xa-G_c",
        "colab_type": "text"
      },
      "source": [
        "Neural network parameters, embedding dimension is 300, we used 4 filters, then batch size and epochs number is set. \n",
        "The last three lines represents the input of the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXTlWnI0-HGi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeding_dim = 300\n",
        "filter_sizes = (1,2,3,4)\n",
        "num_filters = 100\n",
        "batch_size = 64\n",
        "num_epochs = 500\n",
        "\n",
        "input_shape = (max_length,)\n",
        "model_input = Input(shape=input_shape)\n",
        "zz = embedding_layer(model_input)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKCALkoP-HOt",
        "colab_type": "text"
      },
      "source": [
        "Defining the deep neural network model, started by the convolution layers with RELU activation. Then the dropout layer with a rate 0.8. Then the three dense layers separated by a dropout layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C889NfuG-HWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_blocks = []\n",
        "for sz in filter_sizes:\n",
        "    conv = Convolution1D(filters=num_filters,\n",
        "                         kernel_size=sz,\n",
        "                         padding=\"valid\",\n",
        "                         activation=\"relu\",\n",
        "                         strides=1)(zz)\n",
        "    conv = GlobalMaxPooling1D()(conv)\n",
        "    conv_blocks.append(conv)\n",
        "z = Concatenate()(conv_blocks if len(conv_blocks) > 1 else conv_blocks[0])\n",
        "z = Dropout(0.8)(z)\n",
        "model_output = Dense(10, activation=\"sigmoid\" , bias_initializer='zeros')(z)\n",
        "model_output = Dense(10)(model_output)\n",
        "model_output = Dropout(0.8)(model_output)\n",
        "model_output = Dense(2, activation=\"selu\")(model_output)\n",
        "model = Model(model_input, model_output)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ_lxIGL-HeM",
        "colab_type": "text"
      },
      "source": [
        "Model compile, fitting and evaluation. The callback has been used during the experiments. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqQkqKcE-PhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    model.compile(loss=\"categorical_hinge\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    model.summary(85)\n",
        "    #callback = tf.keras.callbacks.EarlyStopping(monitor='val_acc', mode='max', min_delta=1, patience=50)\n",
        "    history = model.fit(Xtrain, y_train, batch_size=batch_size, epochs=50,\n",
        "              validation_data=(Xtest, y_test), verbose=2)\n",
        "    print('History', history.history)\n",
        "    loss, acc = model.evaluate(Xtest, y_test, verbose=2)\n",
        "    print('Test Accuracy: %f' % (acc*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}