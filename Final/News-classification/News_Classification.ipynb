{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "News Classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmarMeriwani/Fake-Financial-News-Detection/blob/master/News_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLkJeKArYJ57",
        "colab_type": "text"
      },
      "source": [
        "# News Classification\n",
        "This document contains the source code for creating the classifier. The dataset uri news aggregator contains 400K news titles with labels for news classification. We used count vectorizer as feature extraction method. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KudA5DqbYKA1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle\n",
        "import nltk.tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pwc9OPjYKGX",
        "colab_type": "text"
      },
      "source": [
        "The code below is used to normalize sentences by removing punbtionations and multiple spaces. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZXFSl_iYKMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_text(s):\n",
        "    s = s.lower()\n",
        "    # remove punctuation that is not word-internal (e.g., hyphens, apostrophes)\n",
        "    s = re.sub('\\s\\W', ' ', s)\n",
        "    s = re.sub('\\W\\s', ' ', s)\n",
        "    # make sure we didn't introduce any double spaces\n",
        "    s = re.sub('\\s+', ' ', s)\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BYaXXoXYKSu",
        "colab_type": "text"
      },
      "source": [
        "Each row in the dataset is processed, normalized, converted to tokens and then all the tokens were converted to a vocabulary set by keeping only the unique words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t68VwVmoYKY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alltokens = []\n",
        "classifiedrows = 400000\n",
        "df3 = pd.DataFrame(columns=['title'])\n",
        "def get_vocabulary(doc,encoding,textIndex,encodeDecode):\n",
        "    #'ISO-8859-1'\n",
        "    df = pd.read_csv(doc, header=0, encoding=encoding)\n",
        "    df = df[:classifiedrows]\n",
        "    atokens = []\n",
        "    for i in range(0,len(df)):\n",
        "        sentence = df.loc[i][textIndex]\n",
        "        sentence = normalize_text(sentence)\n",
        "        if encodeDecode == True:\n",
        "            sentence = sentence.encode('ascii', errors='ignore').decode(\"utf-8\")\n",
        "        df3.loc[i] = sentence\n",
        "        tokens = nltk.tokenize.word_tokenize(sentence)\n",
        "        for t in tokens:\n",
        "            atokens.append(t)\n",
        "    atokens = set(atokens)\n",
        "    return atokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnXq8ESWYKge",
        "colab_type": "text"
      },
      "source": [
        "The second normalization method to work with sentences for classification, it includes:\n",
        "* Removing stop words.\n",
        "* Removing punctuation.\n",
        "* Keeping words with length larger than 1.\n",
        "* Keeping alphabetical tokens only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRHHlBhBYKor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_doc(doc):\n",
        "    # split into tokens by white space\n",
        "    tokens = doc\n",
        "    # remove punctuation from each token\n",
        "    table = str.maketrans('', '', punctuation)\n",
        "    tokens = [w.translate(table) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # filter out stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    # filter out short tokens\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFSLxfNvYKv6",
        "colab_type": "text"
      },
      "source": [
        "Reading the dataset URI news aggregator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKdYPWcoYK3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alltokens = get_vocabulary('uci-news-aggregator.csv','utf-8',1,False)\n",
        "df_store_vocab = pd.DataFrame(columns=['word'])\n",
        "seq = 0\n",
        "for i in alltokens:\n",
        "    df_store_vocab.loc[seq] = i\n",
        "    seq += 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxkzVInPYLEj",
        "colab_type": "text"
      },
      "source": [
        "Creating count vectorizer on the previously acquired vocabulary and then reading the dataset. Finally, creating a dataset for the required fields only (category and title)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU2juaH1YLW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer = CountVectorizer(vocabulary=alltokens)\n",
        "news = pd.read_csv(\"uci-news-aggregator.csv\")\n",
        "news = news[:classifiedrows]\n",
        "seq = 0\n",
        "df2 = pd.DataFrame(columns=['title','category'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA32wNHGYLeI",
        "colab_type": "text"
      },
      "source": [
        "Storing the data from the datasheet to df2 dataset which contains only the required fields."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgVKsotDYLj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(0,len(news)):\n",
        "    sentence = news.loc[i][1]\n",
        "    sentence = normalize_text(sentence)\n",
        "    category = news.loc[i][4]\n",
        "    r = [sentence, category]\n",
        "    df2.loc[seq] = r\n",
        "    seq += 1\n",
        "print(df2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIBK_hrvYLrD",
        "colab_type": "text"
      },
      "source": [
        "Encoding labels and fitting the count vectorizer on the news titles column to create bag of words vector representations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENr_XV7lYLwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = vectorizer.fit_transform(df2['title'])\n",
        "pickle.dump(vectorizer.vocabulary_, open('vocab.pkl', 'wb'))\n",
        "print('SHAPE: ',x.shape)\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(df2['category'])\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3fvAePtYL3H",
        "colab_type": "text"
      },
      "source": [
        "Training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdsV_bPLYL9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mlp = MLPClassifier(activation='tanh', hidden_layer_sizes=(20,20,20))\n",
        "mlp.fit(x_train,y_train)\n",
        "\n",
        "pickle.dump(mlp, open('MLPClassifier4.pkl', 'wb'))\n",
        "score = mlp.score(x_test, y_test)\n",
        "print(score)\n",
        "\n",
        "y2 = mlp.predict(x2)\n",
        "print(encoder.classes_)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}