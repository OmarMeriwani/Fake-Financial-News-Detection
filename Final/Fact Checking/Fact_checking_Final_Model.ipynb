{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fact checking - Final Model.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmarMeriwani/Fake-Financial-News-Detection/blob/master/Fact_checking_Final_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "augc2Vsmq2Ck",
        "colab_type": "text"
      },
      "source": [
        "# Fact checking - Final Model\n",
        "This model combines the other steps of fact checking, which includes company names and ups/downs model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmcgeAmoq2Us",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from stanfordcorenlp import StanfordCoreNLP\n",
        "import os\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import datapath\n",
        "from keras.layers import Embedding\n",
        "from numpy import zeros\n",
        "from string import punctuation\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ho5nTUvq2a9",
        "colab_type": "text"
      },
      "source": [
        "Steps for initializing Stanford Core NLP tool explained [here](https://github.com/OmarMeriwani/Fake-Financial-News-Detection/blob/master/Final/Objectivity/News_Sources_Analysis_Who_Said.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3F4PB-hCq2gH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "java_path = \"C:/Program Files/Java/jdk1.8.0_161/bin/java.exe\"\n",
        "os.environ['JAVAHOME'] = java_path\n",
        "host='http://localhost'\n",
        "port=9000\n",
        "scnlp =StanfordCoreNLP(host, port=port,lang='en', timeout=30000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bfQ55Qpq2lf",
        "colab_type": "text"
      },
      "source": [
        "Getting all the dataset (BusinessTitlesFull) which contains data from (URI news aggregator) dataset, this dataset contains all the 100K news titles that are labeled under \"business\" category.\n",
        "Then we are calling the datasheet of company names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDi9Qcagq2qy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('BusinessTitlesFull.csv',header=0)\n",
        "df2 = pd.DataFrame(columns=['ID','TITLE','URL','PUBLISHER','CATEGORY','HOSTNAME','TIMESTAMP'])\n",
        "df_corp_names = pd.read_csv('UniqueCompanyNames.csv')\n",
        "corp_names = df_corp_names.values.tolist()\n",
        "print(corp_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VtyoEmwq2wI",
        "colab_type": "text"
      },
      "source": [
        "Getting the frequency of each token in company names. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt57czpWq210",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corp_names_freq = {}\n",
        "for i  in corp_names:\n",
        "    tokens = nltk.tokenize.word_tokenize(i[0])\n",
        "    for t in tokens:\n",
        "        if t not in corp_names_freq:\n",
        "            corp_names_freq[str(t).lower()] = 1\n",
        "        else:\n",
        "            corp_names_freq[t] = corp_names_freq.get(str(t).lower()) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tl05RUxZq27N",
        "colab_type": "text"
      },
      "source": [
        "For each news title:\n",
        "* Creating tokens, POS tags, lowercased version of the sentence, and extracting all the words with all upper case letters.\n",
        "* Processing only the titles that contains numbers, either if they were related to stock market points or if it mentions amounts with currencies.\n",
        "* Iterating through all 8K companies, and for each company:\n",
        "  * Convert the full name of the company into tokens.\n",
        "  * Getting company symbol, first word and second word. \n",
        "  * If both the names exist, and they were nouns or if the symbol existed, then the company is detected in the text.\n",
        "* Adding the results found to a dataset that includes, the number amount, company name, and the original IEX name of the company.\n",
        "* The result is stored in the datasheet (CompaniesWithNumbers)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBJY9JgNq3AO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df3 = pd.DataFrame(columns=['corporates','title','number'])\n",
        "seq = 0\n",
        "namesDataset = []\n",
        "NoNeedForData = 1\n",
        "if NoNeedForData == 0:\n",
        "    for i in range(0, 100000):\n",
        "        title= str(df.loc[i].values[1])\n",
        "        '''Two variables to store the company names found within a specific title'''\n",
        "        foundCorporates = ''\n",
        "        foundCorporatesList = []\n",
        "        '''A parameter that is set to 1 to disable checking company names in single words, which may cause wrong detection of company names'''\n",
        "        disable_single_words = 1\n",
        "        \n",
        "        '''Finding POS tags of title, and creating a lowercased version of the title tokens'''\n",
        "        TitleTokens = []\n",
        "        pos_title = scnlp.pos_tag(title)\n",
        "        for p in pos_title:\n",
        "            TitleTokens.append(str(p[0]).lower())\n",
        "        '''Storing all the names with all upper case letters'''\n",
        "        TitleTokensCase = []\n",
        "        for p in pos_title:\n",
        "            TitleTokensCase.append( True if str(p[0])[0].isupper() else False)\n",
        "        \n",
        "        '''The process will continue if the title is related to stock market, and if it mentions points for stock market or if it mentions any currency sign'''\n",
        "        points = re.findall(r'stock\\ |stocks|share\\ |shares', str(title).lower())\n",
        "        if points != []:\n",
        "            dollars = re.findall(r'\\d+\\.?\\d+\\%|[$|£|€|%]\\d+\\.?\\d+\\ ?[bln|billion|b\\ |million|mln|m\\ |k\\ ]?|\\d+\\.?\\d+\\ ?point', title.lower())\n",
        "            if dollars != []:\n",
        "                '''Iterating company names for each news title'''\n",
        "                for corp in corp_names:\n",
        "                    tokens = nltk.tokenize.word_tokenize(corp[0])\n",
        "                    firstOnly = tokens[0]\n",
        "                    both = str(corp[0])\n",
        "                    symbol = str(corp[1])\n",
        "                    bothFound = 0\n",
        "                    for j in range(0, len(TitleTokens)):\n",
        "                        CurrentWord = str(TitleTokens[j]).lower()\n",
        "                        NextWord = ''\n",
        "                        CompanysFirstWord = str(tokens[0]).lower()\n",
        "                        CompanysSecondWord = ''\n",
        "                        try:\n",
        "                            CompanysSecondWord = str(tokens[1]).lower()\n",
        "                        except:\n",
        "                            DONOTHING = 0\n",
        "                        nextPOS = ''\n",
        "                        try:\n",
        "                            NextWord = str(TitleTokens[j + 1]).lower()\n",
        "                            nextPOS = pos_title[j + 1][1]\n",
        "                        except:\n",
        "                            DONOTHING = 0\n",
        "                        currentPOS = ''\n",
        "                        try:\n",
        "                            currentPOS = pos_title[j][1]\n",
        "                        except Exception as e:\n",
        "                            print(e)\n",
        "                            print(pos_title)\n",
        "                        if (CurrentWord == CompanysFirstWord and CompanysSecondWord == '' and TitleTokensCase[j] == True and 'NN' in currentPOS ) or  \\\n",
        "                                (CurrentWord == CompanysFirstWord and NextWord == CompanysSecondWord and NextWord != ''):\n",
        "                            foundCorporates += ' | Corporate Name: '+ both +  ',' + str(j)\n",
        "                            foundCorporatesList.append([str(j), both,'n'])\n",
        "                            bothFound = 1\n",
        "                        if disable_single_words == 0 and bothFound != 1 and CurrentWord == CompanysFirstWord and corp_names_freq.get(CompanysFirstWord) <= 5 \\\n",
        "                                and CompanysFirstWord not in stop_words  and ('NN' in nextPOS ) and 'NN' in currentPOS:\n",
        "                            foundCorporates += ' | One Word:'+ both +  ',' + str(j)\n",
        "                            foundCorporatesList.append([str(j), both,'n'])\n",
        "                        if CurrentWord == symbol and len(symbol) > 3:\n",
        "                            foundCorporates += ' | Corporate Symbol:'+ both + ',' + symbol + ',' + str(j)\n",
        "                            foundCorporatesList.append([str(j), symbol,'s'])\n",
        "                #print(points)\n",
        "                if foundCorporates != '':\n",
        "                    print('FOUND: ',[foundCorporatesList, title,  dollars])\n",
        "                    seq += 1\n",
        "\n",
        "                    df3.loc[seq] = [foundCorporatesList, title,  dollars]\n",
        "    df3.to_csv('CompaniesWithNumbers.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxzOY_G4q3Fq",
        "colab_type": "text"
      },
      "source": [
        "Reading the dataset of CompaniesWithNumbers, in order to start the next process of finding ups and downs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxR7Ovjiq3LL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df4 = pd.read_csv('CompaniesWithNumbers.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9xbgTq2ysQO",
        "colab_type": "text"
      },
      "source": [
        "This method finds if a specific word exists in the vocabulary, then it assigns vector weights extracted from a ready embeddings to make the semantic vector representations. The words that does not exist in the vocabulary are replaced with zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSALp5ZhysYZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_weight_matrix2(embedding, vocab):\n",
        "    vocab_size2 = len(vocab) + 1\n",
        "    weight_matrix = zeros((vocab_size2, 300))\n",
        "    for word, i in vocab:\n",
        "        vector = None\n",
        "        try:\n",
        "            vector = embedding.get_vector(word)\n",
        "        except:\n",
        "            continue\n",
        "        if vector is not None:\n",
        "            weight_matrix[i] = vector\n",
        "    return weight_matrix\n",
        "data = []\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1Q83Cffq3Q6",
        "colab_type": "text"
      },
      "source": [
        "Looping through the dataset that contains news titles, company names and number, then for each news title:\n",
        "* Replace named entities.\n",
        "* Replace numbers.\n",
        "* Remove stop words.\n",
        "* Remove punctuation.\n",
        "* Get POS Tags.\n",
        "* Then create an array of sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY1UfNwmq3Wo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(0,len(df4)):\n",
        "    sentence = df4.loc[i][2]\n",
        "    tokens = scnlp.word_tokenize(sentence)\n",
        "    sentenceList = []\n",
        "    for word in tokens:\n",
        "        isAllUpperCase = True\n",
        "        for letter in word:\n",
        "            if letter.isupper() == False:\n",
        "                isAllUpperCase = False\n",
        "                break\n",
        "\n",
        "        if isAllUpperCase == False:\n",
        "            sentenceList.append(str(word))\n",
        "        else:\n",
        "            sentenceList.append('#ner')\n",
        "    tokens = sentenceList\n",
        "    table = str.maketrans('', '', punctuation)\n",
        "    tokens = [w.translate(table) for w in tokens]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    tokens = [lemmatizer.lemmatize(word.lower()) for word in tokens if len(word) > 1]\n",
        "    sentence = ' '.join(tokens)\n",
        "    NER = scnlp.ner(str(sentence))\n",
        "    POS = scnlp.pos_tag(str(sentence).lower())\n",
        "    sentenceList = []\n",
        "    for i in range(0, len(NER)):\n",
        "        w = NER[i][0]\n",
        "        n = NER[i][1]\n",
        "        pos = NER[i][1]\n",
        "        if str(w).isnumeric() == True:\n",
        "            sentenceList.append('#num')\n",
        "            continue\n",
        "        if pos == 'NNP' and w != '#ner':\n",
        "            sentenceList.append('#ner')\n",
        "            continue\n",
        "        if str(n) == 'O':\n",
        "            sentenceList.append(w)\n",
        "        else:\n",
        "            sentenceList.append('#ner')\n",
        "    sentence = ' '.join(sentenceList)\n",
        "    if sentence.strip() != '':\n",
        "        data.append([sentence])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jUVdBonq3dF",
        "colab_type": "text"
      },
      "source": [
        "Loading ups and downs model, loading google news vectors, converting sentences into semantic vector representations, and finally, running the training and the evaluation processes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "038MwMzMq3iG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = np.array(data)\n",
        "train_docs = data[:, 0]\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_docs)\n",
        "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
        "max_length = 17\n",
        "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "'''================== PLEASE CHANGE THE LINK TO THIS FILE ACCORDING TO THE PATH OF GOOGLE NEWS VECTORS ========================='''\n",
        "wv_from_bin = KeyedVectors.load_word2vec_format(datapath('GoogleNews-vectors-negative300.bin'),\n",
        "                                                binary=True)\n",
        "'''============================================================================================================================='''\n",
        "\n",
        "embedding_vectors = get_weight_matrix2(wv_from_bin, tokenizer.word_index.items())\n",
        "embedding_layer = Embedding(vocab_size, 300, weights=[embedding_vectors], input_length=max_length, trainable=False)\n",
        "model = load_model('upsdowns_model.h5')\n",
        "y = model.predict(Xtrain)\n",
        "for i in range(0, len(y)):\n",
        "    effect = ''\n",
        "    if y[i][0] > y[i][1]:\n",
        "        effect = 'DOWN'\n",
        "    if y[i][0] < y[i][1]:\n",
        "        effect = 'UP'\n",
        "    print(df4.loc[i][2], effect)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}