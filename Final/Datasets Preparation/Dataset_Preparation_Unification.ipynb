{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dataset Preparation - Unification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmarMeriwani/Fake-Financial-News-Detection/blob/master/Dataset_Preparation_Unification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdsckuDYSC0u",
        "colab_type": "text"
      },
      "source": [
        "# Dataset Preparation - Unification\n",
        "In this document, we are unifying fake news from three different sources (politifact, snopes, emergent) and classifying the news to keep only the financial news. \n",
        "Count vectorizer is required to perform the same representations as the ones done in the news classifier. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9W0uJSj7SC7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import sys\n",
        "from urllib.parse import urlparse\n",
        "from dateutil.parser import parse\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from preprocessing import normalize_text\n",
        "print('Started data unification...')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yKI_EPUSDBI",
        "colab_type": "text"
      },
      "source": [
        "For each dataset we are reading different columns sequences and different fake news labels, but to combine them together, the below dataframe (df_main) is created in this way to have five columns: claim, type, label, date and sources (if available). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdTph7HbSDGf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_main = pd.DataFrame(columns=['claim', 'type', 'label', 'date','sources'])\n",
        "print('Reading snopes dataset...')\n",
        "df_snopes = pd.read_csv('snopes.csv')\n",
        "'''\n",
        "0. snopes_page, 1. topic, 2. claim, 3. claim_label, 4. date_published, 5. date_updated, 6. page_url,page_is_example,page_is_image_credit,page_is_archived,page_is_first_citation,tags\n",
        "topic (business), claim, claim_label (FALSE, TRUE, mfalse, mtrue), date_published, page_url\n",
        "'''\n",
        "seq = 0\n",
        "mainseq = 0\n",
        "previous = ''\n",
        "sources = ''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4attxJjSDMa",
        "colab_type": "text"
      },
      "source": [
        "Snopes dataset already contained \"business\" classification, so we only selected the business news. Labels such as mfalse (mostly false) and mtrue (mostly true) were elemenated and converted to true and false. \n",
        "News sources links were parsed separatly, we have selected only the domain name and removed the \"www\" labels. Finally, the resulting features for snopes.com were stored in the main dataframe,.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfZYB7pZSDSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(0, len(df_snopes)):\n",
        "    claim = str(df_snopes.loc[i].values[2])\n",
        "    claim = claim.replace('            SeeÂ Example(s)','')\n",
        "    topic = df_snopes.loc[i].values[1]\n",
        "    if topic != 'business':\n",
        "        continue\n",
        "\n",
        "    date = df_snopes.loc[i].values[4]\n",
        "    dt = parse(date)\n",
        "    label = df_snopes.loc[i].values[3]\n",
        "    if label == 'Unverified' or label == 'mixture':\n",
        "        continue\n",
        "    if label == 'mfalse':\n",
        "        label = 'false'\n",
        "    if label == 'mtrue':\n",
        "        label = 'true'\n",
        "    parsed_uri = urlparse(str(df_snopes.loc[i].values[6]) )\n",
        "    result = '{uri.netloc}'.format(uri=parsed_uri)\n",
        "    result = str(result)\n",
        "    result.replace('www.','')\n",
        "    sources += str(result) + ';'\n",
        "\n",
        "    if claim != previous:\n",
        "\n",
        "        df_main.loc[mainseq] = [claim, topic, label, str(dt.date()) , sources]\n",
        "        print([claim, topic, label, str(dt.date()), sources])\n",
        "        sources = ''\n",
        "        previous = claim\n",
        "        mainseq += 1\n",
        "print(df_main)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v9UDmXsTPZD",
        "colab_type": "text"
      },
      "source": [
        "Emergent and Politifact dataset were not necesarily financial news, therefore, we used the text classification classifier to classify them and select only the business news. The code below shows retrieving the classifier model and the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E_j1bvUTPg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Getting vocabulary...')\n",
        "vec = CountVectorizer(vocabulary=pickle.load(open('../Final/News-classification/vocab.pkl', 'rb')))\n",
        "\n",
        "print('Loading model')\n",
        "file = open(r\"../Final/News-classification/MLPClassifier4.pkl\", 'rb')\n",
        "mlp = pickle.load(file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzUxNXb1TPqQ",
        "colab_type": "text"
      },
      "source": [
        "Reading emergent datasource and selecting claims only, to use them for prediction after storing them in (df_emergent_pred) dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZA_6gvPqTPxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "emergent.csv\n",
        "0.emergent_page, 1.claim,2.claim_description,3.claim_label,4.tags,5.claim_source_domain,\n",
        "6.claim_course_url,7.date,8.body,9.page_domain,10.page_url,11.page_headline,12.page_position,13.page_shares,14.page_order\n",
        "1.claim, 3.claim_label (FALSE, TRUE), 5.claim_source_domain, 7.date\n",
        "'''\n",
        "\n",
        "print('Getting emergent.csv dataset...')\n",
        "df_emergent = pd.read_csv('emergent.csv')\n",
        "df_emergent_pred = pd.DataFrame(columns=['claim'])\n",
        "seq = 0\n",
        "for i in range(0, len(df_emergent)):\n",
        "    claim = str(df_emergent.loc[i].values[1])\n",
        "    claim = normalize_text(claim)\n",
        "    claim = claim.replace('claim: ','')\n",
        "    df_emergent_pred.loc[seq] = [claim]\n",
        "    seq += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUBygehzTP5v",
        "colab_type": "text"
      },
      "source": [
        "Performing the prediction process and storing the results of news classes in the array (y2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_qme6ULTP_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(df_emergent_pred))\n",
        "print('Fitting countvectorizer...')\n",
        "x2 = vec.fit_transform(df_emergent_pred['claim'])\n",
        "print('Prediction...')\n",
        "y2 = mlp.predict(x2)\n",
        "print('Predictions:')\n",
        "seq = 0\n",
        "previous = ''\n",
        "sources = ''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJ2f-AJ3TY0L",
        "colab_type": "text"
      },
      "source": [
        "Redoing the same operations that were done on snopes.com dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR67igTfTY8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(0, len(df_emergent)):\n",
        "    claim = str(df_emergent.loc[i].values[1]).replace('Claim: ','')\n",
        "    topic = y2[i]\n",
        "    if topic != 0:\n",
        "        continue\n",
        "    date = df_emergent.loc[i].values[7]\n",
        "    dt = parse(date)\n",
        "    label = df_emergent.loc[i].values[3]\n",
        "    parsed_uri = urlparse(str(df_emergent.loc[i].values[10]) )\n",
        "    result = '{uri.netloc}'.format(uri=parsed_uri)\n",
        "    result = str(result)\n",
        "    result.replace('www.','')\n",
        "    sources += str(result) + ';'\n",
        "    if claim != previous:\n",
        "        df_main.loc[mainseq] = [claim, topic, label, str(dt.date()) , sources]\n",
        "        print([claim, topic, label, str(dt.date()), sources])\n",
        "        sources = ''\n",
        "        previous = claim\n",
        "        mainseq += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v7RHTI1TZEB",
        "colab_type": "text"
      },
      "source": [
        "Reading politifact dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB-pnQhZTZKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "politifact.csv\n",
        "0.politifact_page, 1.claim, 2.claim_source, 3.claim_citation, 4.claim_label, \n",
        "5.date_published, 6.researched_by, 7.edited_by, 8.tags, 9.page_citation, 10.page_url, 11.page_is_first_citation\n",
        "\n",
        "1.claim, 4.claim_label (barely_true, half-true, mostly-true, pants-fire, FALSE, TRUE), 5.date_published, 10.page_url\n",
        "'''\n",
        "\n",
        "print('Getting politifact.csv dataset...')\n",
        "df_politifact = pd.read_csv('politifact.csv')\n",
        "df_politifact_pred = pd.DataFrame(columns=['claim'])\n",
        "seq = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2xRFJEUTfJb",
        "colab_type": "text"
      },
      "source": [
        "Storing claims of politifact dataset for next prediction step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLtDAU6QTfRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(0, len(df_politifact)):\n",
        "    claim = str(df_politifact.loc[i].values[1])\n",
        "    claim = normalize_text(claim)\n",
        "    df_politifact_pred.loc[seq] = [claim]\n",
        "    seq += 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdxe82f_TiW5",
        "colab_type": "text"
      },
      "source": [
        "Classifying the news."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI5CynBITifh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Fitting countvectorizer...')\n",
        "x2 = vec.fit_transform(df_politifact_pred['claim'])\n",
        "print('Prediction...')\n",
        "y2 = mlp.predict(x2)\n",
        "print('Predictions:')\n",
        "seq = 0\n",
        "previous = ''\n",
        "sources = ''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91Cf8yU1Tm-a",
        "colab_type": "text"
      },
      "source": [
        "Storing the dataset into the main dataframe and selecting only financial news. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR8V6i0jTnG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(0, len(df_politifact)):\n",
        "    claim = str(df_politifact.loc[i].values[1])\n",
        "    topic = y2[i]\n",
        "    if topic != 0:\n",
        "        continue\n",
        "    date = df_politifact.loc[i].values[5]\n",
        "    dt = parse(date)\n",
        "    #print('DATE: ',date, dt.date())\n",
        "    label = df_politifact.loc[i].values[4]\n",
        "    if label == 'pants-fire' or label == 'half-true' or label == 'FALSE':\n",
        "        label = 'false'\n",
        "    else:\n",
        "        label = 'true'\n",
        "    parsed_uri = urlparse(str(df_politifact.loc[i].values[10]))\n",
        "    result = '{uri.netloc}'.format(uri=parsed_uri)\n",
        "    result = str(result)\n",
        "    result.replace('www.','')\n",
        "    sources += str(result) + ';'\n",
        "    if claim != previous:\n",
        "        df_main.loc[mainseq] = [claim, topic, label, str(dt.date()) , sources]\n",
        "        print([claim, topic, label, str(dt.date()), sources])\n",
        "        sources = ''\n",
        "        previous = claim\n",
        "        mainseq += 1\n",
        "        #print(seq)\n",
        "\n",
        "df_main.to_csv('fakenews.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}