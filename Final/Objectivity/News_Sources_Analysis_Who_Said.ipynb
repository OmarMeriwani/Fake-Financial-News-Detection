{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/OmarMeriwani/Fake-Financial-News-Detection/blob/master/News_Sources_Analysis_Who_Said.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gavil3lf_VOf",
    "colab_type": "text"
   },
   "source": [
    "# News Sources Analysis - Who Said\n",
    "This document contains the source code of the \"Who Said\" classifier. Please refer to the main read me document to find the prequistics to run the document.\n",
    "The below code shows the required libraries to run the solution, which include nltk methods for preprocessing tasks and wordnet; sk_learn libraries for multiple layer perceptron and splitting training and testing samples; and the library for Stanford Core NLP. In addition to pandas, pickle and os libraris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "FerK4LyK_YDp",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import os\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem.porter import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEuguEHl_YTU",
    "colab_type": "text"
   },
   "source": [
    "These lines are often repeated in other parts of the project. Since Stanford Core NLP tool requires a Java server to be running inside the host, then the below code initiates the requirements for Stanford tool to be running, it includes port number, host name (localhost) and the path to Java in the PC, it is not necesary that this path is the same in all PCs.\n",
    "After creating the object of Stanford tool, we initiated different NLTK tools that are:  toenizer, stemmer and a lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "QyYN67S0_Yal",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "java_path = \"C:/Program Files/Java/jdk1.8.0_161/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "host='http://localhost'\n",
    "port=9000\n",
    "scnlp =StanfordCoreNLP(host, port=port,lang='en', timeout=30000)\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\s+|\\:|\\.', gaps=True)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alKPP3LKB6GA",
    "colab_type": "text"
   },
   "source": [
    "In this steps we are reading the training dataset (Using Resources Dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "EHgao8KOB6Ny",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFmFVC0c_Yg4",
    "colab_type": "text"
   },
   "source": [
    "The below method is used to get verb synonyms from WordNet. It is used only to get \"say\" verb synonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "9sLqtwnD_Ymn",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def getVerbs(verb):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(verb):\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(l.name())\n",
    "    return set(synonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-w0KJvk_Ysh",
    "colab_type": "text"
   },
   "source": [
    "The below method exploits parsers to get the noun that is used with the dependency parser in Stanford tool to get the subject of the verb, specifically to get \"Who Said\", the task was done by using POS tags and dependency parser, both of them are Stanford Core NLP tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "0A2T4AWR_YyP",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "def WhoSaid (sent, verb):\n",
    "    result = []\n",
    "    '''Getting POS tags and depedency parse for the sentence'''\n",
    "    deps = scnlp.dependency_parse(sent)\n",
    "    tags = scnlp.pos_tag(sent)\n",
    "    '''Creating an array that will store all the say verbs in the sentence'''\n",
    "    verbindex = []\n",
    "    for i in range(1, len(tags)):\n",
    "        if tags[i][0] == verb:\n",
    "            verbindex .append( i + 1)\n",
    "    '''After storing say verbs, the subject for each verb will be selected and added to the results array'''\n",
    "    for i in deps:\n",
    "        if i[1] in verbindex and i[0] == 'nsubj':\n",
    "            result.append([tags[i[2] - 1][0], tags[i[2] - 1][1], ners[i[2] - 1][1] ])\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFPAS7A-_Y4P",
    "colab_type": "text"
   },
   "source": [
    "The below code represents the process of feature extraction, in the beginning, the dataset is read and two arrays declared for the labels and the features. Features are in two type, ones that are related to the say verb which are extracted by using parsers, and ones that are related to the colon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "cKBOELRo_Y9U",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Using Resources Dataset.csv',header=0)\n",
    "seq = 0\n",
    "x = []\n",
    "y = []\n",
    "for i in range(0,len(df)):\n",
    "    '''Getting news title and removing frequently occuring punctuation'''\n",
    "    title= str(df.loc[i].values[0])\n",
    "    title = title.replace('...','')\n",
    "    '''Performing POS tagging on the title sentence'''\n",
    "    lemTags = scnlp.pos_tag(title)\n",
    "    '''Getting labels from the datasource'''\n",
    "    isreferenced = df.loc[i].values[2]\n",
    "    \n",
    "    '''Preparing set of features for each news title'''\n",
    "    colonAvailable = 1 if (title.find(':') != -1) else 0\n",
    "    tags = scnlp.pos_tag(title)\n",
    "    tagsarr = []\n",
    "    sayverbs = getVerbs('say')\n",
    "    isSayVerb = 0\n",
    "    isNPPSaid = 0\n",
    "    isNERSaid = 0\n",
    "    isQuestion = 0\n",
    "    nnpfound = 0\n",
    "    if '?' in title:\n",
    "        isQuestion = 1\n",
    "    nnp_followed_by_colon = 0\n",
    "    mid = int((len(tags) -1) / 2)\n",
    "    \n",
    "    '''Using who said method to get the subject, and then applying POS tagging and Named entity recognition on the sentence to check if the subjects are proper nouns or named entities'''\n",
    "    for t in lemTags:\n",
    "        verb = stemmer.stem(str(t[0]).lower())\n",
    "        if 'V' in t[1]:\n",
    "            #sayverbs is an array that contains all the synonyms of the verb say, these verbs are retrieved by the method above\n",
    "            for j in sayverbs:\n",
    "                if verb == str(j).lower():\n",
    "                    #Getting subject of the verb\n",
    "                    whosaid = WhoSaid(title, str(t[0]))\n",
    "                    if whosaid != []:\n",
    "                        for w in whosaid:\n",
    "                            #Checking if the subject is a proper noun or a named entity\n",
    "                            if w[1] == 'NNP' and isNPPSaid == 0:\n",
    "                                isNPPSaid = 1\n",
    "                            if w[2] != 'O' and isNERSaid == 0:\n",
    "                                isNERSaid = 1\n",
    "                        print('Whosaid', whosaid)\n",
    "                    isSayVerb = 1\n",
    "                \n",
    "                    print('SayVerb',t[0])\n",
    "                    break\n",
    "    '''After the last step, three features are acquired, isNPPSaid which refers to a proper noun subject of the verb; \n",
    "    isNERSaid which refers to a named entity verb subject; and isSayVerb where a say verb exists\n",
    "    \n",
    "    The features below are related to the colon in the sentence, and whether it is followed or preceeded by a proper noun'''\n",
    "    for i in range(0,mid):\n",
    "        word = tags[i][1]\n",
    "        if nnpfound == 1 and word == ':':\n",
    "            nnp_followed_by_colon = 1\n",
    "            break\n",
    "        if word == 'NNP':\n",
    "            nnpfound = 1\n",
    "        else:\n",
    "            nnpfound = 0\n",
    "    nnp_preceeded_by_colon = 0\n",
    "    for i in range(0,mid ):\n",
    "        word = tags[len(tags) -1 - i][1]\n",
    "        word2 = tags[len(tags) -1 - i][0]\n",
    "        if  word == 'NNP':\n",
    "            nnpfound = 1\n",
    "        if nnpfound == 1 and word == ':':\n",
    "            nnp_preceeded_by_colon = 1\n",
    "            break\n",
    "        if word != 'NNP':\n",
    "            nnpfound = 0\n",
    "    print(title)\n",
    "    print( 'isreferenced', isreferenced,'colonAvailable', colonAvailable, 'nnp_followed_by_colon:',\n",
    "           nnp_followed_by_colon,'nnp_preceeded_by_colon',nnp_preceeded_by_colon, 'isNPPSaid',isNPPSaid,\n",
    "           'isNERSaid',isNERSaid, 'isQuestion',isQuestion)\n",
    "    x.append([colonAvailable,nnp_followed_by_colon,nnp_preceeded_by_colon, isNPPSaid,isNERSaid, isQuestion])\n",
    "    y.append(isreferenced)\n",
    "    print('--------------------------------------------------------------------------')\n",
    "max = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ltIV-Km_ZD_",
    "colab_type": "text"
   },
   "source": [
    "After extracting the features, the two resulting arrays are seprated into training and test, and then the evaluation performed using a multiple layer perceptron classifier. Finally the resulting model is stored to be used in the objectivity test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "tLXeY4rD_ZMk",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.33, random_state=42)\n",
    "for i in range(0,100):\n",
    "    mlp = MLPClassifier()\n",
    "    mlp.fit(X_train,y_train)\n",
    "    score = mlp.score(X_test,y_test)\n",
    "    print(score)\n",
    "    if score > max:\n",
    "        max = score\n",
    "        pickle.dump(mlp, open('WhoSaid.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xbld-X7_ZSV",
    "colab_type": "text"
   },
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "News Sources Analysis - Who Said",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
