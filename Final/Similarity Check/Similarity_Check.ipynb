{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Similarity Check.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmarMeriwani/Fake-Financial-News-Detection/blob/master/Similarity_Check.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJEkB8taK1QM",
        "colab_type": "text"
      },
      "source": [
        "# Similarity Check\n",
        "This document contains the source code of similarity check in news groups as part of the fake financial news detection framework."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgzK_z43K1YX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from string import punctuation\n",
        "import pandas as pd\n",
        "from numpy import zeros\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import datapath\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.stem.porter import *\n",
        "from keras.utils import np_utils\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHfUh5cLK1e2",
        "colab_type": "text"
      },
      "source": [
        "Defining lemmatizer and stemmer to be used in the  next steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R27mWV__K1ky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdz6dgG3K1q4",
        "colab_type": "text"
      },
      "source": [
        "Normalization method that performs tokenization, and then removes punctuation and stop words and finally performs the case folding and the lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okskLMCbK1wz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_doc(doc):\n",
        "    doc = doc.encode('ascii', errors='ignore').decode(\"utf-8\")\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # remove punctuation from each token\n",
        "    table = str.maketrans('', '', punctuation)\n",
        "    tokens = [w.translate(table) for w in tokens]\n",
        "    # filter out stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    # filter out short tokens\n",
        "    tokens = [lemmatizer.lemmatize(word.lower()) for word in tokens if len(word) > 1]\n",
        "    return tokens\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObYbHwTrK12u",
        "colab_type": "text"
      },
      "source": [
        "A method that returns text from a file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7izp-ClK18Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_doc(filename):\n",
        "    file = open(filename, 'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UetiErR-K2CU",
        "colab_type": "text"
      },
      "source": [
        "Line by line clearning for lines in a specific document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ultHTxhkK2Io",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def doc_to_clean_lines(doc):\n",
        "    clean_lines = ''\n",
        "    lines = doc.splitlines()\n",
        "\n",
        "    for line in lines:\n",
        "        clean_lines = ' '.join(clean_doc(line))\n",
        "    return clean_lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLLer-cYK2Oj",
        "colab_type": "text"
      },
      "source": [
        "This method finds if a specific word exists in the vocabulary, then it assigns vector weights extracted from a ready embeddings to make the semantic vector representations. The words that does not exist in the vocabulary are replaced with zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah4WwEXvK2hQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_weight_matrix2(embedding, vocab):\n",
        "    vocab_size2 = len(vocab) + 1\n",
        "    weight_matrix = zeros((vocab_size2, 300))\n",
        "    for word, i in vocab:\n",
        "        vector = None\n",
        "        try:\n",
        "            vector = embedding.get_vector(word)\n",
        "        except:\n",
        "            continue\n",
        "        if vector is not None:\n",
        "            weight_matrix[i] = vector\n",
        "    return weight_matrix\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE7leduvK2nK",
        "colab_type": "text"
      },
      "source": [
        "This method reads a file and then creates a dataset of normalized sentences, timestamps and the groups (which represents the training labels in this experiment)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ_dgkaXK2s5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readfile(filename):\n",
        "    df = pd.read_csv(filename,header=0)\n",
        "    mode = 'sentence'\n",
        "    data = []\n",
        "    prev = ''\n",
        "    for i in range(0,len(df)):\n",
        "        sentence = str(df.loc[i][1])\n",
        "\n",
        "        group = int(df.loc[i][2])\n",
        "        timestamp = df.loc[i][3]\n",
        "        sentence = doc_to_clean_lines(sentence)\n",
        "        data.append([sentence,timestamp,group])\n",
        "    return data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4urDI-NK2zY",
        "colab_type": "text"
      },
      "source": [
        "This method splits data to test and training datasets in a way that divides the groups between the two sets according to the percentage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOUHXB-HLsbd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sortByGroup(val):\n",
        "    return val[2]\n",
        "def split(docs, percentage):\n",
        "  \n",
        "    docs.sort(key=sortByGroup)\n",
        "    length = len(docs)\n",
        "    groups = []\n",
        "    test = []\n",
        "    training = []\n",
        "    previousGroup = 0\n",
        "    for i in docs:\n",
        "        if i[2] != previousGroup and previousGroup == 0:\n",
        "            previousGroup = i[2]\n",
        "            groups.append([i[0],i[2]])\n",
        "        if i[2] == previousGroup and previousGroup != 0:\n",
        "            groups.append([i[0],i[2]])\n",
        "        if i[2] != previousGroup and previousGroup != 0:\n",
        "            gLength = groups.__len__()\n",
        "            testsize = int(gLength * percentage)\n",
        "            '''After collecting all the samples of a specific group, we used train_test_split method from sklearn to divide them'''\n",
        "            groupsTraining, groupsTest  = train_test_split(groups,test_size=percentage)\n",
        "            for t in groupsTraining:\n",
        "                training.append(t)\n",
        "            for t in groupsTest:\n",
        "                test.append(t)\n",
        "            groups = []\n",
        "            groups.append([i[0], i[2]])\n",
        "            previousGroup = i[2]\n",
        "            print(i[2])\n",
        "\n",
        "    firstlength = int (length * percentage)\n",
        "    return training,test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cSEh6s-LshW",
        "colab_type": "text"
      },
      "source": [
        "Reading the dataset, applying split method and the previous normalization methods, then convert the text into sequences and padding the sequences. The labels (groups) are converted into categorical arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT3flhAcLsmf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = readfile('NewsGroups1300.csv')\n",
        "traindata, testdata = split(data,0.2)\n",
        "traindata = np.array(traindata)\n",
        "testdata = np.array(testdata)\n",
        "train_docs = traindata[:,0]\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_docs)\n",
        "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
        "\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "ytrain = traindata[:,1]\n",
        "ytrain = np_utils.to_categorical(ytrain)\n",
        "test_docs = testdata[:,0]\n",
        "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
        "\n",
        "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "ytest = testdata[:,1]\n",
        "\n",
        "ytest = np_utils.to_categorical(ytest)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7Ip2lwpLsrp",
        "colab_type": "text"
      },
      "source": [
        "Load google news embeddings and convert the sequences into embedding vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Uep_zRGLsw2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wv_from_bin = KeyedVectors.load_word2vec_format(datapath('E:/Data/GN/GoogleNews-vectors-negative300.bin'), binary=True)\n",
        "embedding_vectors = get_weight_matrix2(wv_from_bin, tokenizer.word_index.items())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s88-YedWLs2j",
        "colab_type": "text"
      },
      "source": [
        "Creating the neural network model which is a sequential model with two dense layers with 1400, 741 units respectivly  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S3cQPMxLs7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_layer = Embedding(vocab_size, 300, weights=[embedding_vectors], input_length=max_length, trainable=False)\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Dense(1400, activation='relu', input_dim=200))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(741, activation='softmax'))\n",
        "import tensorflow as tf\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.compat.v1.keras.losses.categorical_crossentropy,\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42NQ-UhCL98G",
        "colab_type": "text"
      },
      "source": [
        "Training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBHbNrD8L-Fp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(Xtrain, ytrain, epochs=20, verbose=2, validation_data=(Xtest, ytest))\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}