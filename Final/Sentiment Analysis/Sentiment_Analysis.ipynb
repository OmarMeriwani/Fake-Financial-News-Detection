{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmarMeriwani/Fake-Financial-News-Detection/blob/master/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC3l5FjrHLZo",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Analysis\n",
        "This document contains the source code for the sentiment analysis model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B51ZqXizHLgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from string import punctuation\n",
        "import pandas as pd\n",
        "from numpy import zeros\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential, Model\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import datapath\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.stem.porter import *\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding, GlobalMaxPooling1D\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.utils import np_utils\n",
        "import os\n",
        "from stanfordcorenlp import StanfordCoreNLP\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "java_path = \"C:/Program Files/Java/jdk1.8.0_161/bin/java.exe\"\n",
        "os.environ['JAVAHOME'] = java_path\n",
        "host='http://localhost'\n",
        "port=9000\n",
        "scnlp =StanfordCoreNLP(host, port=port,lang='en', timeout=30000)\n",
        "stemmer = PorterStemmer()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trpGMg7KHLlo",
        "colab_type": "text"
      },
      "source": [
        "Normalization steps, it includes tokenization, removing punctuation and stop words, and finally lemmatizing and lowercasing the words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ML1fE2lHLrD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_doc(doc):\n",
        "    doc = doc.encode('ascii', errors='ignore').decode(\"utf-8\")\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # remove punctuation from each token\n",
        "    table = str.maketrans('', '', punctuation)\n",
        "    tokens = [w.translate(table) for w in tokens]\n",
        "    # filter out stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "    # filter out short tokens\n",
        "    tokens = [lemmatizer.lemmatize(word.lower()) for word in tokens if len(word) > 1 and str(word).isalpha() == True ]\n",
        "    return tokens\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ud0YEnnHLwK",
        "colab_type": "text"
      },
      "source": [
        "This method finds if a specific word exists in the vocabulary, then it assigns vector weights extracted from a ready embeddings to make the semantic vector representations. The words that does not exist in the vocabulary are replaced with zeros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2F1IEsHHL1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_weight_matrix2(embedding, vocab):\n",
        "    vocab_size2 = len(vocab) + 1\n",
        "    weight_matrix = zeros((vocab_size2, 300))\n",
        "    for word, i in vocab:\n",
        "        vector = None\n",
        "        try:\n",
        "            vector = embedding.get_vector(word)\n",
        "        except:\n",
        "            continue\n",
        "        if vector is not None:\n",
        "            weight_matrix[i] = vector\n",
        "    return weight_matrix\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y9LDyxZHL7E",
        "colab_type": "text"
      },
      "source": [
        "The method below reads a datasheet file, and performs the following tasks:\n",
        "* Get the claim and the label.\n",
        "* Replace named entities.\n",
        "* Replace numbers.\n",
        "* Remove stop words.\n",
        "* Remove punctuation.\n",
        "* Get POS Tags.\n",
        "* Then create an array of sentences and labels\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBBHPcfbHMAj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readfile(filename):\n",
        "    df = pd.read_csv(filename,header=0)\n",
        "\n",
        "    data = pd.DataFrame(columns=['claim','label'])\n",
        "    seq = 0\n",
        "    table = str.maketrans('', '', punctuation)\n",
        "\n",
        "    for i in range(0,len(df)):\n",
        "        sentence = str(df.loc[i][1])\n",
        "        tokens = scnlp.word_tokenize(sentence)\n",
        "        sentenceList = []\n",
        "        for word in tokens:\n",
        "            isAllUpperCase = True\n",
        "            for letter in word:\n",
        "                if letter.isupper() == False:\n",
        "                    isAllUpperCase = False\n",
        "                    break\n",
        "\n",
        "            if isAllUpperCase == False:\n",
        "                sentenceList.append(str(word))\n",
        "            else:\n",
        "                sentenceList.append('#ner')\n",
        "        tokens = sentenceList\n",
        "\n",
        "        tokens = [w.translate(table) for w in tokens]\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [w for w in tokens if not w in stop_words]\n",
        "        # filter out short tokens\n",
        "        tokens = [lemmatizer.lemmatize(word.lower()) for word in tokens if len(word) > 1]\n",
        "        sentence = ' '.join(tokens)\n",
        "        NER = scnlp.ner(str(sentence))\n",
        "        POS = scnlp.pos_tag(str(sentence).lower())\n",
        "        sentenceList = []\n",
        "        for i in range(0,len(NER)):\n",
        "            w = NER[i][0]\n",
        "            n = NER[i][1]\n",
        "            pos = NER[i][1]\n",
        "            if str(w).isnumeric() == True:\n",
        "                sentenceList.append('#num')\n",
        "                continue\n",
        "            if pos == 'NNP' and w != '#ner':\n",
        "                sentenceList.append('#ner')\n",
        "                continue\n",
        "            if str(n) == 'O' :\n",
        "                sentenceList.append(w)\n",
        "            else:\n",
        "                sentenceList.append('#ner')\n",
        "        sentence = ' '.join(sentenceList)\n",
        "        label = int(df.loc[i][4])\n",
        "        if sentence.strip() != '':\n",
        "            data.loc[seq] = [sentence,label]\n",
        "            print(sentence, label)\n",
        "            seq += 1\n",
        "    return data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wClZtUM-HMFe",
        "colab_type": "text"
      },
      "source": [
        "Using the previous method, the data is loaded and separated into claims and labels. Note that it is required to download the dataset train.csv from the [link](https://www.kaggle.com/c/fake-news), the other dataset FakeNewsSA.csv already exists in the path, but it requires to change the indices of readfile method before changing the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOlU2BloHMK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = readfile('Kaggle Competition/train.csv')\n",
        "#data = readfile('FakeNewsSA.csv')\n",
        "claims = data[['claim']]\n",
        "labels = data[['label']]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gnx5fH1_HMQW",
        "colab_type": "text"
      },
      "source": [
        "Splitting the resulting sets to training and test, and converting labels into categorical set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTSFUGWkHMVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(claims,labels,test_size=0.2)\n",
        "traindata = np.array(x_train)\n",
        "testdata = np.array(x_test)\n",
        "\n",
        "y_testold = y_test\n",
        "y_test = np_utils.to_categorical(y_test,num_classes=2)\n",
        "print(y_testold, y_test)\n",
        "y_train = np_utils.to_categorical(y_train,num_classes=2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kinbv2RTHMae",
        "colab_type": "text"
      },
      "source": [
        "Selecting the required fields from test and training claims and converting them into sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mBGUwLlHMfy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_docs = traindata[:,0]\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_docs)\n",
        "test_docs = testdata[:,0]\n",
        "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sxbHv1zHMk9",
        "colab_type": "text"
      },
      "source": [
        "Limiting the length of the resulting seqeunces and converting them into embedding vectors using Google news embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VKTPdmVHMqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "print('max_length', max_length)\n",
        "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "\n",
        "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
        "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "\n",
        "# define vocabulary size (largest integer value)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# load embedding from file\n",
        "wv_from_bin = KeyedVectors.load_word2vec_format(datapath('E:/Data/GN/GoogleNews-vectors-negative300.bin'), binary=True)\n",
        "embedding_vectors = get_weight_matrix2(wv_from_bin, tokenizer.word_index.items())\n",
        "\n",
        "print('embedding_vectors.shape() =============================')\n",
        "print(embedding_vectors.shape)\n",
        "\n",
        "# create the embedding layer\n",
        "embedding_layer = Embedding(vocab_size, 300, weights=[embedding_vectors], input_length=max_length, trainable=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPMu-MUJHMvP",
        "colab_type": "text"
      },
      "source": [
        "Deep neural network parameters and creating input layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FahIvXpHM0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeding_dim = 300\n",
        "filter_sizes = (1,2,3,4)\n",
        "num_filters = 100\n",
        "dropout_prob = (0.0, 0.5)\n",
        "batch_size = 64\n",
        "num_epochs = 500\n",
        "print('max_length',max_length)\n",
        "input_shape = (max_length,)\n",
        "model_input = Input(shape=input_shape)\n",
        "zz = embedding_layer(model_input)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnb5z5CKHM5u",
        "colab_type": "text"
      },
      "source": [
        "Creating the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXqb0fPKHM_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_blocks = []\n",
        "for sz in filter_sizes:\n",
        "    conv = Convolution1D(filters=num_filters,\n",
        "                         kernel_size=sz,\n",
        "                         padding=\"valid\",\n",
        "                         activation=\"relu\",\n",
        "                         strides=1)(zz)\n",
        "    conv = GlobalMaxPooling1D()(conv)\n",
        "    conv_blocks.append(conv)\n",
        "z = Concatenate()(conv_blocks if len(conv_blocks) > 1 else conv_blocks[0])\n",
        "z = Dropout(0.8)(z)\n",
        "model_output = Dense(10, activation=\"sigmoid\" , bias_initializer='zeros')(z)\n",
        "model_output = Dense(10)(model_output)\n",
        "model_output = Dropout(0.8)(model_output)\n",
        "model_output = Dense(2, activation=\"selu\")(model_output)\n",
        "model = Model(model_input, model_output)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdU2M-RiIXRX",
        "colab_type": "text"
      },
      "source": [
        "Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mO-ASNdPIXYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=\"categorical_hinge\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.summary(85)\n",
        "history = model.fit(Xtrain, y_train, batch_size=batch_size, epochs=50,\n",
        "    validation_data=(Xtest, y_test), verbose=2)\n",
        "print('History', history.history)\n",
        "loss, acc = model.evaluate(Xtest, y_test, verbose=2)\n",
        "print('Test Accuracy: %f' % (acc*100))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}